{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from deepsvdd.train import TrainerDeepSVDD\n",
    "from deepsvdd.test import eval\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336/4336: [==============================>.] - ETA 0.4s\n",
      "Pretraining Autoencoder... Epoch: 0, Loss: 160.952\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 1, Loss: 132.630\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 2, Loss: 108.652\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 3, Loss: 88.486\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 4, Loss: 72.101\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 5, Loss: 59.252\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 6, Loss: 49.158\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 7, Loss: 41.215\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 8, Loss: 35.098\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 9, Loss: 30.323\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 10, Loss: 26.621\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 11, Loss: 23.687\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 12, Loss: 21.316\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 13, Loss: 19.392\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 14, Loss: 17.804\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 15, Loss: 16.470\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 16, Loss: 15.330\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 17, Loss: 14.344\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 18, Loss: 13.493\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 19, Loss: 12.747\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 20, Loss: 12.093\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 21, Loss: 11.523\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 22, Loss: 11.006\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 23, Loss: 10.553\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 24, Loss: 10.146\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 25, Loss: 9.765\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 26, Loss: 9.430\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 27, Loss: 9.117\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 28, Loss: 8.823\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 29, Loss: 8.561\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 30, Loss: 8.314\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 31, Loss: 8.083\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 32, Loss: 7.864\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 33, Loss: 7.658\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 34, Loss: 7.467\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 35, Loss: 7.279\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 36, Loss: 7.106\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 37, Loss: 6.944\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 38, Loss: 6.787\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 39, Loss: 6.643\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 40, Loss: 6.512\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 41, Loss: 6.385\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 42, Loss: 6.260\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 43, Loss: 6.145\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 44, Loss: 6.031\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 45, Loss: 5.924\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 46, Loss: 5.824\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 47, Loss: 5.727\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 48, Loss: 5.635\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 49, Loss: 5.548\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 50, Loss: 5.494\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 51, Loss: 5.484\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 52, Loss: 5.476\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 53, Loss: 5.469\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 54, Loss: 5.460\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 55, Loss: 5.454\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 56, Loss: 5.442\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 57, Loss: 5.432\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 58, Loss: 5.424\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 59, Loss: 5.415\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 60, Loss: 5.407\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 61, Loss: 5.400\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 62, Loss: 5.387\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 63, Loss: 5.381\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 64, Loss: 5.372\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 65, Loss: 5.360\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 66, Loss: 5.347\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 67, Loss: 5.341\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 68, Loss: 5.328\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 69, Loss: 5.321\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 70, Loss: 5.311\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 71, Loss: 5.303\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 72, Loss: 5.293\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 73, Loss: 5.284\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 74, Loss: 5.273\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 75, Loss: 5.261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 76, Loss: 5.251\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 77, Loss: 5.238\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 78, Loss: 5.234\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 79, Loss: 5.222\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 80, Loss: 5.213\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 81, Loss: 5.200\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 82, Loss: 5.193\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 83, Loss: 5.185\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 84, Loss: 5.173\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 85, Loss: 5.161\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 86, Loss: 5.146\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 87, Loss: 5.141\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 88, Loss: 5.127\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 89, Loss: 5.119\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 90, Loss: 5.113\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 91, Loss: 5.098\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 92, Loss: 5.087\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 93, Loss: 5.074\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 94, Loss: 5.065\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 95, Loss: 5.058\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 96, Loss: 5.047\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 97, Loss: 5.034\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 98, Loss: 5.023\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 99, Loss: 5.010\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 100, Loss: 4.999\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 101, Loss: 4.991\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 102, Loss: 4.979\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 103, Loss: 4.967\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 104, Loss: 4.958\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 105, Loss: 4.948\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 106, Loss: 4.937\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 107, Loss: 4.925\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 108, Loss: 4.914\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 109, Loss: 4.899\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 110, Loss: 4.887\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 111, Loss: 4.881\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 112, Loss: 4.864\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 113, Loss: 4.854\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 114, Loss: 4.843\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 115, Loss: 4.835\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 116, Loss: 4.821\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 117, Loss: 4.811\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 118, Loss: 4.801\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 119, Loss: 4.791\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 120, Loss: 4.777\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 121, Loss: 4.765\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 122, Loss: 4.754\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 123, Loss: 4.744\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 124, Loss: 4.730\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 125, Loss: 4.722\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 126, Loss: 4.709\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 127, Loss: 4.700\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 128, Loss: 4.685\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 129, Loss: 4.675\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 130, Loss: 4.666\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 131, Loss: 4.652\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 132, Loss: 4.641\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 133, Loss: 4.628\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 134, Loss: 4.621\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 135, Loss: 4.605\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 136, Loss: 4.601\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 137, Loss: 4.584\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 138, Loss: 4.573\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 139, Loss: 4.563\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 140, Loss: 4.547\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 141, Loss: 4.544\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 142, Loss: 4.531\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 143, Loss: 4.517\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 144, Loss: 4.505\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 145, Loss: 4.491\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 146, Loss: 4.483\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 147, Loss: 4.472\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 148, Loss: 4.459\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Pretraining Autoencoder... Epoch: 149, Loss: 4.451\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=150\n",
    "    num_epochs_ae=150\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    weight_decay=0.5e-6\n",
    "    weight_decay_ae=0.5e-3\n",
    "    lr_ae=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=200\n",
    "    pretrain=True\n",
    "    latent_dim=32\n",
    "    anormal_class=5\n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader_train, dataloader_val, dataloader_test = get_mnist(args)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, dataloader_train, dataloader_val, device)\n",
    "\n",
    "if args.pretrain:\n",
    "    deep_SVDD.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 0, Loss: 1.433\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 0, Loss: 0.989\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.668\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 1, Loss: 0.45\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.336\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 2, Loss: 0.274\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.203\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 3, Loss: 0.182\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.138\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 4, Loss: 0.122\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.102\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 5, Loss: 0.0928\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.080\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 6, Loss: 0.0772\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.067\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 7, Loss: 0.0631\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.057\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 8, Loss: 0.0559\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.050\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 9, Loss: 0.0488\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.045\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 10, Loss: 0.0439\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.040\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 11, Loss: 0.0394\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.037\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 12, Loss: 0.0366\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.034\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 13, Loss: 0.0339\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.032\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 14, Loss: 0.0313\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.030\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 15, Loss: 0.0294\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.028\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 16, Loss: 0.0276\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.026\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 17, Loss: 0.0256\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.024\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 18, Loss: 0.0244\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.023\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 19, Loss: 0.0236\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.022\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 20, Loss: 0.0219\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.021\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 21, Loss: 0.0209\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.020\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 22, Loss: 0.0201\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.019\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 23, Loss: 0.019\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.018\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 24, Loss: 0.0182\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.018\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 25, Loss: 0.0177\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.017\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 26, Loss: 0.0171\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.016\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 27, Loss: 0.0163\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.016\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 28, Loss: 0.0157\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.015\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 29, Loss: 0.0151\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.015\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 30, Loss: 0.0145\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.014\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 31, Loss: 0.0141\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.014\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 32, Loss: 0.0135\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.013\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 33, Loss: 0.0133\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.013\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 34, Loss: 0.0129\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.013\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 35, Loss: 0.0125\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.013\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 36, Loss: 0.0124\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.012\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 37, Loss: 0.0123\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.012\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 38, Loss: 0.0113\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.011\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 39, Loss: 0.0115\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.011\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 40, Loss: 0.0107\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.011\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 41, Loss: 0.0106\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.010\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 42, Loss: 0.0103\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.010\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 43, Loss: 0.01\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.010\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 44, Loss: 0.00987\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.010\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 45, Loss: 0.00959\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.010\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 46, Loss: 0.00952\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.009\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 47, Loss: 0.00912\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.009\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 48, Loss: 0.00894\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.009\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 49, Loss: 0.0087\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 50, Loss: 0.00851\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.009\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 51, Loss: 0.00851\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 52, Loss: 0.00841\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.009\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 53, Loss: 0.00851\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 54, Loss: 0.00851\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 55, Loss: 0.00846\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 56, Loss: 0.00846\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 57, Loss: 0.0084\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 58, Loss: 0.00833\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 59, Loss: 0.00834\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 60, Loss: 0.0084\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 61, Loss: 0.00844\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 62, Loss: 0.00825\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 63, Loss: 0.00825\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 64, Loss: 0.00821\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 65, Loss: 0.00825\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 66, Loss: 0.00824\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 67, Loss: 0.00815\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 68, Loss: 0.00816\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 69, Loss: 0.00816\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 70, Loss: 0.00802\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 71, Loss: 0.00812\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 72, Loss: 0.00808\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 73, Loss: 0.00798\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 74, Loss: 0.00809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 75, Loss: 0.00803\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 76, Loss: 0.008\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 77, Loss: 0.00788\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 78, Loss: 0.00791\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 79, Loss: 0.00784\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 80, Loss: 0.00789\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 81, Loss: 0.00784\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 82, Loss: 0.00795\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 83, Loss: 0.00782\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 84, Loss: 0.00785\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 85, Loss: 0.00782\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 86, Loss: 0.00783\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 87, Loss: 0.00769\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 88, Loss: 0.00766\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 89, Loss: 0.00768\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 90, Loss: 0.00772\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 91, Loss: 0.00768\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 92, Loss: 0.00763\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 93, Loss: 0.00758\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 94, Loss: 0.0076\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 95, Loss: 0.0076\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 96, Loss: 0.00759\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.008\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 97, Loss: 0.00749\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 98, Loss: 0.00757\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 99, Loss: 0.00751\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 100, Loss: 0.00756\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 101, Loss: 0.00736\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 102, Loss: 0.0074\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 103, Loss: 0.00733\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 104, Loss: 0.00734\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 105, Loss: 0.0073\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 106, Loss: 0.00726\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 107, Loss: 0.00727\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 108, Loss: 0.00727\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 109, Loss: 0.00719\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 110, Loss: 0.00715\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 111, Loss: 0.00712\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 112, Loss: 0.00713\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 113, Loss: 0.00717\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 114, Loss: 0.0071\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 115, Loss: 0.00707\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 116, Loss: 0.00703\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 117, Loss: 0.00703\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 118, Loss: 0.007\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 119, Loss: 0.00689\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 120, Loss: 0.00694\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 121, Loss: 0.00689\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 122, Loss: 0.00695\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 123, Loss: 0.00689\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 124, Loss: 0.00681\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 125, Loss: 0.0069\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 126, Loss: 0.00679\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 127, Loss: 0.00689\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 128, Loss: 0.00682\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 129, Loss: 0.00673\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 130, Loss: 0.00669\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 131, Loss: 0.00665\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 132, Loss: 0.00667\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 133, Loss: 0.00666\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 134, Loss: 0.00657\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 135, Loss: 0.00667\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 136, Loss: 0.00658\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 137, Loss: 0.00656\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 138, Loss: 0.00651\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 139, Loss: 0.00651\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.007\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 140, Loss: 0.00654\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 141, Loss: 0.0064\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 142, Loss: 0.00648\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 143, Loss: 0.00643\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 144, Loss: 0.00633\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 145, Loss: 0.0063\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 146, Loss: 0.00635\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 147, Loss: 0.0063\n",
      "Weights saved.\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 148, Loss: 0.00638\n",
      "4336/4336: [==============================>.] - ETA 0.1s\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.006\n",
      "1085/1085: [==========================>.....] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 149, Loss: 0.00623\n",
      "Weights saved.\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 0.908\n"
     ]
    }
   ],
   "source": [
    "labels1, labels2, scores = eval(deep_SVDD.net, deep_SVDD.c, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in = scores[labels1==0]\n",
    "scores_out = scores[labels1==1]\n",
    "\n",
    "scores_ELL = scores[labels2==1]\n",
    "scores_TDE = scores[labels2==2]\n",
    "scores_SNIIb = scores[labels2==3]\n",
    "scores_WRayot = scores[labels2==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f03e2bc05c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in[scores_in<20], bins=10, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_out, bins=8, color='r', alpha=0.3, density=True, label='Outlier')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/lib/histograms.py:823: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f03e2a77080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in, bins=20, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_ELL, bins=10, color='r', alpha=0.3, density=True, label='ELL')\n",
    "plt.hist(scores_TDE, bins=1, color='g', alpha=0.3, density=True, label='TDE')\n",
    "plt.hist(scores_SNIIb, bins=10, color='y', alpha=0.3, density=True, label='SNIIb')\n",
    "plt.hist(scores_WRayot, bins=2, color='k', alpha=0.3, density=True, label='WRayot')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
