{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dagmm.train import TrainerDAGMM\n",
    "from dagmm.test import eval\n",
    "from preprocess import get_ALeRCE_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  128/27058: [>...............................] - ETA 0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/ALeRCE-detector-per/dagmm/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 1.2sss\n",
      "Training DAGMM... Epoch: 0, Loss: 18.373\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 18.485\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 18.402\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 18.570\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 4, Loss: 18.131\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 18.254\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 18.129\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 7, Loss: 18.562\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 8, Loss: 18.535\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 9, Loss: 18.429\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 18.644\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 11, Loss: 18.524\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 18.588\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 13, Loss: 18.379\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 18.157\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 18.432\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 18.368\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 17, Loss: 18.396\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 18, Loss: 18.165\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 19, Loss: 18.372\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 20, Loss: 18.600\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 18.586\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 22, Loss: 18.392\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 23, Loss: 18.560\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 24, Loss: 18.455\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 25, Loss: 18.441\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 26, Loss: 18.642\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 27, Loss: 18.386\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 28, Loss: 18.594\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 29, Loss: 18.260\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 18.417\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 31, Loss: 18.529\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 32, Loss: 18.379\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 33, Loss: 18.182\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 18.285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 35, Loss: 18.364\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 36, Loss: 18.259\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 18.401\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 18.302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 18.437\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 40, Loss: 18.400\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 18.286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 18.518\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 18.615\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 18.178\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 45, Loss: 18.313\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 18.293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 18.386\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 48, Loss: 18.572\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 49, Loss: 18.434\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 50, Loss: 18.222\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 51, Loss: 18.293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 52, Loss: 18.438\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 53, Loss: 18.585\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 18.312\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 55, Loss: 18.538\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 56, Loss: 18.441\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 18.374\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 18.241\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 18.540\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 18.517\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 18.257\n",
      "27058/27058: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 62, Loss: 18.370\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 63, Loss: 18.484\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 18.418\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 65, Loss: 18.491\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 18.375\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 67, Loss: 18.586\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 68, Loss: 18.358\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 69, Loss: 18.517\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 70, Loss: 18.486\n",
      "27058/27058: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 71, Loss: 18.084\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 18.055\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 73, Loss: 18.335\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 18.433\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 18.407\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 18.595\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 77, Loss: 18.658\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 78, Loss: 18.401\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 79, Loss: 18.593\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 80, Loss: 18.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 81, Loss: 18.459\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 82, Loss: 18.704\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 83, Loss: 18.746\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 84, Loss: 18.494\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 85, Loss: 18.131\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 86, Loss: 18.345\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 87, Loss: 18.245\n",
      "27058/27058: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 88, Loss: 18.554\n",
      "27058/27058: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 89, Loss: 18.508\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 90, Loss: 18.504\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 91, Loss: 18.181\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 92, Loss: 18.507\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 93, Loss: 18.456\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 18.508\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 95, Loss: 18.140\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 18.313\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 97, Loss: 18.823\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 98, Loss: 18.416\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 18.660\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 100, Loss: 18.169\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 18.436\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 18.547\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 103, Loss: 18.397\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 104, Loss: 18.308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 105, Loss: 18.561\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 18.586\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 107, Loss: 18.512\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 108, Loss: 18.106\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 109, Loss: 18.144\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 110, Loss: 18.536\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 111, Loss: 18.444\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 18.707\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 18.414\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 18.348\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 18.400\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 18.415\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 117, Loss: 18.373\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 18.375\n",
      "27058/27058: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 119, Loss: 18.755\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 120, Loss: 18.226\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 121, Loss: 18.557\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 18.233\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 18.210\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 124, Loss: 18.106\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 125, Loss: 18.437\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 18.145\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 127, Loss: 18.477\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 18.400\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 129, Loss: 18.363\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 130, Loss: 18.209\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 131, Loss: 18.435\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 132, Loss: 18.489\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 133, Loss: 18.439\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 18.184\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 135, Loss: 18.363\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 136, Loss: 18.316\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 18.301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 138, Loss: 18.256\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 139, Loss: 18.329\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 140, Loss: 18.451\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 18.335\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 18.448\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 143, Loss: 18.208\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 144, Loss: 18.432\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 145, Loss: 18.078\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 18.279\n",
      "27058/27058: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 147, Loss: 18.310\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 148, Loss: 18.530\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 149, Loss: 18.247\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 150, Loss: 18.236\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 151, Loss: 18.578\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 152, Loss: 18.514\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 153, Loss: 18.505\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 154, Loss: 18.234\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 155, Loss: 18.588\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 156, Loss: 18.509\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 157, Loss: 18.249\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 158, Loss: 18.138\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 159, Loss: 18.621\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 160, Loss: 18.293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 161, Loss: 18.558\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 162, Loss: 18.575\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 163, Loss: 18.373\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 18.560\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 18.086\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 166, Loss: 18.193\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 18.309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 18.489\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 18.273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 18.421\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 18.431\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 18.433\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 173, Loss: 18.532\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 18.547\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 175, Loss: 18.322\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 176, Loss: 18.421\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 177, Loss: 18.522\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 18.445\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 18.634\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 180, Loss: 18.084\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 181, Loss: 18.337\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 182, Loss: 18.525\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 183, Loss: 18.497\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 184, Loss: 18.448\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 185, Loss: 18.582\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 18.550\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 187, Loss: 18.454\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 18.164\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 189, Loss: 18.064\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 190, Loss: 18.219\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 191, Loss: 18.250\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 192, Loss: 18.488\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 18.585\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 194, Loss: 18.734\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 18.416\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 196, Loss: 18.217\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 18.381\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 198, Loss: 18.450\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 18.597\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    batch_size = 128\n",
    "    num_epochs = 200\n",
    "    n_gmm = 3\n",
    "    lr = 1e-4\n",
    "    lambda_energy = 0.1\n",
    "    lambda_cov = 0.005\n",
    "    latent_dim = 1\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "args = Args() # Parsing all the arguments for the training\n",
    "\n",
    "dataloader_train, scaler, classes = get_ALeRCE_data(args.batch_size, 'train', mode='train')\n",
    "dataloader_test, _, _ = get_ALeRCE_data(args.batch_size, 'test', mode='test', scaler=scaler)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, dataloader_train, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/ALeRCE-detector-per/dagmm/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.0000, Recall : 0.0000, F-score : 0.0000\n",
      "ROC AUC score: 50.90\n"
     ]
    }
   ],
   "source": [
    "labels1, labels2, scores, latent = eval(dagmm.model, (dataloader_train, dataloader_test), device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in = scores[labels1==0]\n",
    "scores_out = scores[labels1==1]\n",
    "\n",
    "scores_ELL = scores[labels2==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff3ee640e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in, bins=50, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_out, bins=20, color='r', alpha=0.3, density=True, label='Outlier')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
