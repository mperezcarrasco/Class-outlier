{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from deepsvdd.train import TrainerDeepSVDD\n",
    "from deepsvdd.test import eval\n",
    "from preprocess import get_ALeRCE_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 0, Loss: 0.187\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 0, Loss: 0.168\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.170\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 1, Loss: 0.152\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.155\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 2, Loss: 0.137\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.141\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 3, Loss: 0.124\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.128\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 4, Loss: 0.113\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.117\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 5, Loss: 0.102\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.106\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 6, Loss: 0.0922\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.097\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 7, Loss: 0.0843\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.088\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 8, Loss: 0.0759\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.080\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 9, Loss: 0.0685\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.073\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 10, Loss: 0.0624\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.066\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 11, Loss: 0.0553\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.059\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 12, Loss: 0.0498\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.054\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 13, Loss: 0.0452\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.048\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 14, Loss: 0.0409\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.043\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 15, Loss: 0.0371\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.039\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 16, Loss: 0.0326\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.035\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 17, Loss: 0.0292\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.031\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 18, Loss: 0.0261\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.028\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 19, Loss: 0.0238\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.025\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 20, Loss: 0.0209\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.023\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 21, Loss: 0.0187\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.020\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 22, Loss: 0.0167\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.018\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 23, Loss: 0.0153\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.016\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 24, Loss: 0.0139\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.015\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 25, Loss: 0.0126\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.013\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 26, Loss: 0.0112\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.012\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 27, Loss: 0.0101\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.011\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 28, Loss: 0.00905\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.010\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 29, Loss: 0.00845\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.009\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 30, Loss: 0.00779\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.009\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 31, Loss: 0.00753\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.008\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 32, Loss: 0.00686\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.007\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 33, Loss: 0.00626\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.007\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 34, Loss: 0.00581\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.007\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 35, Loss: 0.00573\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.006\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 36, Loss: 0.00542\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.006\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 37, Loss: 0.00507\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.005\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 38, Loss: 0.00481\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.005\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 39, Loss: 0.0047\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.005\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 40, Loss: 0.00446\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.005\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 41, Loss: 0.00424\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.005\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 42, Loss: 0.00412\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 43, Loss: 0.00391\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 44, Loss: 0.00383\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 45, Loss: 0.00374\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 46, Loss: 0.00362\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 47, Loss: 0.00345\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 48, Loss: 0.00341\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 49, Loss: 0.00324\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.004\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 50, Loss: 0.00318\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 51, Loss: 0.00324\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 52, Loss: 0.00316\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 53, Loss: 0.00292\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 54, Loss: 0.00299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 55, Loss: 0.00287\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 56, Loss: 0.00273\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 57, Loss: 0.00276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 58, Loss: 0.00257\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 59, Loss: 0.00262\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 60, Loss: 0.00246\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 61, Loss: 0.00253\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 62, Loss: 0.00241\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.003\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 63, Loss: 0.00235\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 64, Loss: 0.00219\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 65, Loss: 0.00214\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 66, Loss: 0.00222\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 67, Loss: 0.00218\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 68, Loss: 0.00214\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 69, Loss: 0.00209\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 70, Loss: 0.00194\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 71, Loss: 0.00201\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 72, Loss: 0.00193\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 73, Loss: 0.00186\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 74, Loss: 0.00184\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 75, Loss: 0.00178\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 76, Loss: 0.00179\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 77, Loss: 0.00174\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 78, Loss: 0.00168\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 79, Loss: 0.00164\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 80, Loss: 0.00157\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 81, Loss: 0.00161\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 82, Loss: 0.00149\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 83, Loss: 0.00145\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 84, Loss: 0.00154\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 85, Loss: 0.00145\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.002\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 86, Loss: 0.00147\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 87, Loss: 0.00143\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 88, Loss: 0.00142\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 89, Loss: 0.00137\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 90, Loss: 0.0013\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 91, Loss: 0.00131\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 92, Loss: 0.00131\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 93, Loss: 0.0012\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 94, Loss: 0.00118\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 95, Loss: 0.00117\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 96, Loss: 0.00118\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 97, Loss: 0.00112\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 98, Loss: 0.00112\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 99, Loss: 0.00107\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 100, Loss: 0.00107\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 101, Loss: 0.00109\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 102, Loss: 0.00101\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 103, Loss: 0.000997\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 104, Loss: 0.00102\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 105, Loss: 0.000944\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 106, Loss: 0.000915\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 107, Loss: 0.000945\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 108, Loss: 0.00092\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 109, Loss: 0.000886\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 110, Loss: 0.000829\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 111, Loss: 0.000817\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 112, Loss: 0.000821\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 113, Loss: 0.000787\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 114, Loss: 0.0008\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 115, Loss: 0.000755\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 116, Loss: 0.000735\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 117, Loss: 0.000725\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 118, Loss: 0.000766\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 119, Loss: 0.000709\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 120, Loss: 0.000686\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 121, Loss: 0.000683\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 122, Loss: 0.000663\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 123, Loss: 0.000648\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 124, Loss: 0.00065\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 125, Loss: 0.000653\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 126, Loss: 0.000621\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 127, Loss: 0.000627\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 128, Loss: 0.000599\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 129, Loss: 0.000615\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 130, Loss: 0.000554\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 131, Loss: 0.000556\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 132, Loss: 0.000574\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 133, Loss: 0.00055\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 134, Loss: 0.000538\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 135, Loss: 0.000512\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 136, Loss: 0.000515\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 137, Loss: 0.000498\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 138, Loss: 0.000499\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.001\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 139, Loss: 0.00047\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 140, Loss: 0.000457\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 141, Loss: 0.000467\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 142, Loss: 0.000448\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 143, Loss: 0.000459\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 144, Loss: 0.000436\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 145, Loss: 0.000434\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 146, Loss: 0.000439\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 147, Loss: 0.000408\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 148, Loss: 0.000397\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 149, Loss: 0.000398\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 150, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 150, Loss: 0.000399\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 151, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 151, Loss: 0.000396\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 152, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 152, Loss: 0.000398\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 153, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 153, Loss: 0.000416\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 154, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 154, Loss: 0.000394\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 155, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 155, Loss: 0.000398\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 156, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 156, Loss: 0.000382\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 157, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 157, Loss: 0.000388\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 158, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 158, Loss: 0.000426\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 159, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 159, Loss: 0.000393\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 160, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 160, Loss: 0.000394\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 161, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 161, Loss: 0.00038\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 162, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 162, Loss: 0.000389\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 163, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 163, Loss: 0.000389\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 164, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 164, Loss: 0.000399\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 165, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 165, Loss: 0.000373\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 166, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 166, Loss: 0.000413\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 167, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 167, Loss: 0.000384\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 168, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 168, Loss: 0.000381\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 169, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 169, Loss: 0.000384\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 170, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 170, Loss: 0.000381\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 171, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 171, Loss: 0.000401\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 172, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 172, Loss: 0.000389\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 173, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 173, Loss: 0.000384\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 174, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 174, Loss: 0.000385\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 175, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 175, Loss: 0.000383\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 176, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 176, Loss: 0.000402\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 177, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 177, Loss: 0.000378\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 178, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 178, Loss: 0.000375\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 179, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 179, Loss: 0.00037\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 180, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 180, Loss: 0.000379\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 181, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 181, Loss: 0.000374\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 182, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 182, Loss: 0.000381\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 183, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 183, Loss: 0.000375\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 184, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 184, Loss: 0.000364\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 185, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 185, Loss: 0.000361\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 186, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 186, Loss: 0.000361\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 187, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 187, Loss: 0.000351\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 188, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 188, Loss: 0.000367\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 189, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 189, Loss: 0.000394\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 190, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 190, Loss: 0.000346\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 191, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 191, Loss: 0.000353\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 192, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 192, Loss: 0.00038\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 193, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 193, Loss: 0.000356\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 194, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 194, Loss: 0.000354\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 195, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 195, Loss: 0.00035\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 196, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 196, Loss: 0.000361\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 197, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 197, Loss: 0.000363\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 198, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 198, Loss: 0.000375\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 199, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 199, Loss: 0.000359\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 200, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 200, Loss: 0.000369\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 201, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 201, Loss: 0.000361\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 202, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 202, Loss: 0.00036\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 203, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 203, Loss: 0.000359\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 204, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 204, Loss: 0.000367\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 205, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 205, Loss: 0.000378\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 206, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 206, Loss: 0.000351\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 207, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 207, Loss: 0.000333\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 208, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 208, Loss: 0.000365\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 209, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 209, Loss: 0.000354\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 210, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 210, Loss: 0.00036\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 211, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 211, Loss: 0.000345\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 212, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 212, Loss: 0.000343\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 213, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 213, Loss: 0.000343\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 214, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 214, Loss: 0.000355\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 215, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 215, Loss: 0.000355\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 216, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 216, Loss: 0.000337\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 217, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 217, Loss: 0.000347\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 218, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 218, Loss: 0.000343\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 219, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 219, Loss: 0.000362\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 220, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 220, Loss: 0.000342\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 221, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 221, Loss: 0.000355\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 222, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 222, Loss: 0.000347\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 223, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 223, Loss: 0.000323\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 224, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 224, Loss: 0.000342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 225, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 225, Loss: 0.000344\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 226, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 226, Loss: 0.000347\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 227, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 227, Loss: 0.000338\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 228, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 228, Loss: 0.00033\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 229, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 229, Loss: 0.000346\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 230, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 230, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 231, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 231, Loss: 0.000351\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 232, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 232, Loss: 0.000329\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 233, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 233, Loss: 0.000339\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 234, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 234, Loss: 0.00032\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 235, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 235, Loss: 0.000337\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 236, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 236, Loss: 0.000317\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 237, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 237, Loss: 0.000333\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 238, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 238, Loss: 0.000318\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 239, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 239, Loss: 0.00033\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 240, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 240, Loss: 0.000328\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 241, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 241, Loss: 0.000328\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 242, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 242, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 243, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 243, Loss: 0.000317\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 244, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 244, Loss: 0.000325\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 245, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 245, Loss: 0.000321\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 246, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 246, Loss: 0.000332\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 247, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 247, Loss: 0.000335\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 248, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 248, Loss: 0.000333\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 249, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 249, Loss: 0.000329\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 250, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 250, Loss: 0.000333\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 251, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 251, Loss: 0.000323\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 252, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 252, Loss: 0.000325\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 253, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 253, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 254, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 254, Loss: 0.000337\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 255, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 255, Loss: 0.000316\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 256, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 256, Loss: 0.000314\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 257, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 257, Loss: 0.000324\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 258, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 258, Loss: 0.00031\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 259, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 259, Loss: 0.000305\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 260, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 260, Loss: 0.000338\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 261, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 261, Loss: 0.00032\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 262, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 262, Loss: 0.000322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 263, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 263, Loss: 0.000297\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 264, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 264, Loss: 0.000305\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 265, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 265, Loss: 0.000304\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 266, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 266, Loss: 0.000318\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 267, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 267, Loss: 0.000324\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 268, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 268, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 269, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 269, Loss: 0.000302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 270, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 270, Loss: 0.000322\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 271, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 271, Loss: 0.000309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 272, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 272, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 273, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 273, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 274, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 274, Loss: 0.000319\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 275, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 275, Loss: 0.000305\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 276, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 276, Loss: 0.000324\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 277, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 277, Loss: 0.000308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 278, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 278, Loss: 0.000315\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 279, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 279, Loss: 0.000306\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 280, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 280, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 281, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 281, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 282, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 282, Loss: 0.000321\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 283, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 283, Loss: 0.000319\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 284, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 284, Loss: 0.000312\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 285, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 285, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 286, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 286, Loss: 0.000307\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 287, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 287, Loss: 0.00028\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 288, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 288, Loss: 0.000294\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 289, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 289, Loss: 0.000312\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 290, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 290, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 291, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 291, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 292, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 292, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 293, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 293, Loss: 0.000312\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 294, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 294, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 295, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 295, Loss: 0.000306\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 296, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 296, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 297, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 297, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 298, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 298, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 299, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 299, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 300, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 300, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 301, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 301, Loss: 0.000313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 302, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 302, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 303, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 303, Loss: 0.00031\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 304, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 304, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 305, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 305, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 306, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 306, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 307, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 307, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 308, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 308, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 309, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 309, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 310, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 310, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 311, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 311, Loss: 0.000309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 312, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 312, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 313, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 313, Loss: 0.000274\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 314, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 314, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 315, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 315, Loss: 0.000302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 316, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 316, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 317, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 317, Loss: 0.000305\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 318, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 318, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 319, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 319, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 320, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 320, Loss: 0.000304\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 321, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 321, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 322, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 322, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 323, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 323, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 324, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 324, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 325, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 325, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 326, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 326, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 327, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 327, Loss: 0.000312\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 328, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 328, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 329, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 329, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 330, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 330, Loss: 0.000309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 331, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 331, Loss: 0.000305\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 332, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 332, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 333, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 333, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 334, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 334, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 335, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 335, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 336, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 336, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 337, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 337, Loss: 0.000308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 338, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 338, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 339, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 339, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 340, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 340, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 341, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 341, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 342, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 342, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 343, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 343, Loss: 0.000316\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 344, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 344, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 345, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 345, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 346, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 346, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 347, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 347, Loss: 0.000318\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 348, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 348, Loss: 0.000323\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 349, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 349, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 350, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 350, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 351, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 351, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 352, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 352, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 353, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 353, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 354, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 354, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 355, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 355, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 356, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 356, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 357, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 357, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 358, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 358, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 359, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 359, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 360, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 360, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 361, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 361, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 362, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 362, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 363, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 363, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 364, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 364, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 365, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 365, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 366, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 366, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 367, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 367, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 368, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 368, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 369, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 369, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 370, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 370, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 371, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 371, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 372, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 372, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 373, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 373, Loss: 0.000267\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 374, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 374, Loss: 0.00031\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 375, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 375, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 376, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 376, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 377, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 377, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 378, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 378, Loss: 0.000319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 379, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 379, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 380, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 380, Loss: 0.000295\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 381, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 381, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 382, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 382, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 383, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 383, Loss: 0.000307\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 384, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 384, Loss: 0.000304\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 385, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 385, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 386, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 386, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 387, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 387, Loss: 0.000295\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 388, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 388, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 389, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 389, Loss: 0.000308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 390, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 390, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 391, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 391, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 392, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 392, Loss: 0.000308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 393, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 393, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 394, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 394, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 395, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 395, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 396, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 396, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 397, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 397, Loss: 0.000306\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 398, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 398, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 399, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 399, Loss: 0.000307\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 400, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 400, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 401, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 401, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 402, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 402, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 403, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 403, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 404, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 404, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 405, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 405, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 406, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 406, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 407, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 407, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 408, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 408, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 409, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 409, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 410, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 410, Loss: 0.000295\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 411, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 411, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 412, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 412, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 413, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 413, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 414, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 414, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 415, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 415, Loss: 0.000277\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 416, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 416, Loss: 0.000277\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 417, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 417, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 418, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 418, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 419, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 419, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 420, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 420, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 421, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 421, Loss: 0.00031\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 422, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 422, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 423, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 423, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 424, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 424, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 425, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 425, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 426, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 426, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 427, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 427, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 428, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 428, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 429, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 429, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 430, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 430, Loss: 0.000302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 431, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 431, Loss: 0.000305\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 432, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 432, Loss: 0.000268\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 433, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 433, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 434, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 434, Loss: 0.000265\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 435, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 435, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 436, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 436, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 437, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 437, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 438, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 438, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 439, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 439, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 440, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 440, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 441, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 441, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 442, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 442, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 443, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 443, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 444, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 444, Loss: 0.000275\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 445, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 445, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 446, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 446, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 447, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 447, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 448, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 448, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 449, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 449, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 450, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 450, Loss: 0.000266\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 451, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 451, Loss: 0.000314\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 452, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 452, Loss: 0.000302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 453, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 453, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 454, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 454, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 455, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 455, Loss: 0.000305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 456, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 456, Loss: 0.00027\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 457, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 457, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 458, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 458, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 459, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 459, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 460, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 460, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 461, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 461, Loss: 0.000306\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 462, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 462, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 463, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 463, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 464, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 464, Loss: 0.000313\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 465, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 465, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 466, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 466, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 467, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 467, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 468, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 468, Loss: 0.00027\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 469, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 469, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 470, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 470, Loss: 0.000275\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 471, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 471, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 472, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 472, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 473, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 473, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 474, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 474, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 475, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 475, Loss: 0.000295\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 476, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 476, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 477, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 477, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 478, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 478, Loss: 0.000277\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 479, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 479, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 480, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 480, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 481, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 481, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 482, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 482, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 483, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 483, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 484, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 484, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 485, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 485, Loss: 0.000309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 486, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 486, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 487, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 487, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 488, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 488, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 489, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 489, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 490, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 490, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 491, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 491, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 492, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 492, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 493, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 493, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 494, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 494, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 495, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 495, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 496, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 496, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 497, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 497, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 498, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 498, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 499, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 499, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 500, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 500, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 501, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 501, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 502, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 502, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 503, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 503, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 504, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 504, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 505, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 505, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 506, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 506, Loss: 0.000301\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 507, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 507, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 508, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 508, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 509, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 509, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 510, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 510, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 511, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 511, Loss: 0.000271\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 512, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 512, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 513, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 513, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 514, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 514, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 515, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 515, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 516, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 516, Loss: 0.000326\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 517, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 517, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 518, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 518, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 519, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 519, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 520, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 520, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 521, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 521, Loss: 0.000276\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 522, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 522, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 523, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 523, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 524, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 524, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 525, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 525, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 526, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 526, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 527, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 527, Loss: 0.000294\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 528, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 528, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 529, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 529, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 530, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 530, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 531, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 531, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 532, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 532, Loss: 0.000289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 533, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 533, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 534, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 534, Loss: 0.000264\n",
      "Weights saved.\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 535, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 535, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 536, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 536, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 537, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 537, Loss: 0.000275\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 538, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 538, Loss: 0.000272\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 539, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 539, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 540, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 540, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 541, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 541, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 542, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 542, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 543, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 543, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 544, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 544, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 545, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 545, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 546, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 546, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 547, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 547, Loss: 0.000277\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 548, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 548, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 549, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 549, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 550, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 550, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 551, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 551, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 552, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 552, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 553, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 553, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 554, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 554, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 555, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 555, Loss: 0.000269\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 556, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 556, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 557, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 557, Loss: 0.000271\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 558, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 558, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 559, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 559, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 560, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 560, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 561, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 561, Loss: 0.000285\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 562, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 562, Loss: 0.000282\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 563, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 563, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 564, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 564, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 565, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 565, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 566, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 566, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 567, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 567, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 568, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 568, Loss: 0.000314\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 569, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 569, Loss: 0.00032\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 570, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 570, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 571, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 571, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 572, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 572, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 573, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 573, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 574, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 574, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 575, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 575, Loss: 0.000266\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 576, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 576, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 577, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 577, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 578, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 578, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 579, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 579, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 580, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 580, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 581, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 581, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 582, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 582, Loss: 0.000288\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 583, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 583, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 584, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 584, Loss: 0.000296\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 585, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 585, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 586, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 586, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 587, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 587, Loss: 0.0003\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 588, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 588, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 589, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 589, Loss: 0.000302\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 590, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 590, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 591, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 591, Loss: 0.000277\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 592, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 592, Loss: 0.000309\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 593, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 593, Loss: 0.000274\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 594, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 594, Loss: 0.000294\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 595, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 595, Loss: 0.000289\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 596, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 596, Loss: 0.000294\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 597, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 597, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 598, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 598, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 599, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 599, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 600, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 600, Loss: 0.000297\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 601, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 601, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 602, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 602, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 603, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 603, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 604, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 604, Loss: 0.000298\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 605, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 605, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 606, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 606, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 607, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 607, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 608, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 608, Loss: 0.000268\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 609, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 609, Loss: 0.00029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 610, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 610, Loss: 0.000281\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 611, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 611, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 612, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 612, Loss: 0.000292\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 613, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 613, Loss: 0.000279\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 614, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 614, Loss: 0.00028\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 615, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 615, Loss: 0.000273\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 616, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 616, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 617, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 617, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 618, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 618, Loss: 0.000284\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 619, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 619, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 620, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 620, Loss: 0.000311\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 621, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 621, Loss: 0.000299\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 622, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 622, Loss: 0.000275\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 623, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 623, Loss: 0.00029\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 624, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 624, Loss: 0.000303\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 625, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 625, Loss: 0.000287\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 626, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 626, Loss: 0.000269\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 627, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 627, Loss: 0.000308\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 628, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 628, Loss: 0.000278\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 629, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 629, Loss: 0.000286\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 630, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 630, Loss: 0.000291\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 631, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 631, Loss: 0.000293\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 632, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 632, Loss: 0.000283\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 633, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 633, Loss: 0.000272\n",
      "27058/27058: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 634, Loss: 0.000\n",
      "4741/4741: [===============================>] - ETA 0.0s\n",
      "Testing Autoencoder... Epoch: 634, Loss: 0.000268\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=1000\n",
    "    patience=100\n",
    "    lr=1e-6\n",
    "    weight_decay= 0.5e-1\n",
    "    lr_milestones=[150, 300, 450, 600]\n",
    "    batch_size=128\n",
    "    pretrain=True\n",
    "    latent_dim=32   \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataloader_train, scaler, classes = get_ALeRCE_data(args.batch_size, 'train', mode='train')\n",
    "dataloader_val, _, _ = get_ALeRCE_data(args.batch_size, 'val', mode='test', scaler=scaler)\n",
    "dataloader_test, _, _ = get_ALeRCE_data(args.batch_size, 'test', mode='test', scaler=scaler)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, dataloader_train, dataloader_val, device)\n",
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 0.655\n"
     ]
    }
   ],
   "source": [
    "labels1, labels2, scores = eval(deep_SVDD.net, deep_SVDD.c, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in = scores[labels1==0]\n",
    "scores_out = scores[labels1==1]\n",
    "\n",
    "scores_ELL = scores[labels2==1]\n",
    "scores_TDE = scores[labels2==2]\n",
    "scores_SNIIb = scores[labels2==3]\n",
    "scores_WRayot = scores[labels2==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdf6e05ca20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in[scores_in<20], bins=10, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_out, bins=8, color='r', alpha=0.3, density=True, label='Outlier')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/lib/histograms.py:823: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdf6c363588>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in, bins=20, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_ELL, bins=10, color='r', alpha=0.3, density=True, label='ELL')\n",
    "plt.hist(scores_TDE, bins=1, color='g', alpha=0.3, density=True, label='TDE')\n",
    "plt.hist(scores_SNIIb, bins=10, color='y', alpha=0.3, density=True, label='SNIIb')\n",
    "plt.hist(scores_WRayot, bins=2, color='k', alpha=0.3, density=True, label='WRayot')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
