{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from deepsvdd.train import TrainerDeepSVDD\n",
    "from deepsvdd.test import eval\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.4ss\n",
      "Pretraining Autoencoder... Epoch: 0, Loss: 78.547\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 1, Loss: 16.994\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 2, Loss: 7.832\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 3, Loss: 5.055\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 4, Loss: 3.795\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 5, Loss: 3.085\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 6, Loss: 2.637\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 7, Loss: 2.330\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 8, Loss: 2.106\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 9, Loss: 1.937\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 10, Loss: 1.803\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 11, Loss: 1.694\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 12, Loss: 1.602\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 13, Loss: 1.527\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 14, Loss: 1.460\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 15, Loss: 1.404\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 16, Loss: 1.354\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 17, Loss: 1.311\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 18, Loss: 1.273\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 19, Loss: 1.238\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 20, Loss: 1.207\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 21, Loss: 1.180\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 22, Loss: 1.155\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 23, Loss: 1.130\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 24, Loss: 1.112\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 25, Loss: 1.092\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 26, Loss: 1.075\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 27, Loss: 1.059\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 28, Loss: 1.044\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 29, Loss: 1.029\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 30, Loss: 1.018\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 31, Loss: 1.006\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 32, Loss: 0.995\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 33, Loss: 0.983\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 34, Loss: 0.974\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 35, Loss: 0.964\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 36, Loss: 0.955\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 37, Loss: 0.945\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 38, Loss: 0.938\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 39, Loss: 0.927\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 40, Loss: 0.920\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 41, Loss: 0.912\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 42, Loss: 0.905\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 43, Loss: 0.896\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 44, Loss: 0.890\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 45, Loss: 0.882\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 46, Loss: 0.877\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 47, Loss: 0.871\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 48, Loss: 0.865\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 49, Loss: 0.859\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 50, Loss: 0.853\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 51, Loss: 0.849\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 52, Loss: 0.844\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 53, Loss: 0.839\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 54, Loss: 0.836\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 55, Loss: 0.832\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 56, Loss: 0.826\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 57, Loss: 0.821\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 58, Loss: 0.819\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 59, Loss: 0.815\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 60, Loss: 0.813\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 61, Loss: 0.808\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 62, Loss: 0.804\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 63, Loss: 0.801\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 64, Loss: 0.799\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 65, Loss: 0.796\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 66, Loss: 0.794\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 67, Loss: 0.790\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 68, Loss: 0.787\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 69, Loss: 0.785\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 70, Loss: 0.783\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 71, Loss: 0.781\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 72, Loss: 0.779\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 73, Loss: 0.774\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 74, Loss: 0.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 75, Loss: 0.771\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 76, Loss: 0.770\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 77, Loss: 0.768\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 78, Loss: 0.768\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 79, Loss: 0.763\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 80, Loss: 0.761\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 81, Loss: 0.760\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 82, Loss: 0.756\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 83, Loss: 0.757\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 84, Loss: 0.755\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 85, Loss: 0.753\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 86, Loss: 0.752\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 87, Loss: 0.750\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 88, Loss: 0.748\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 89, Loss: 0.747\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 90, Loss: 0.743\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 91, Loss: 0.743\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 92, Loss: 0.742\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 93, Loss: 0.741\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 94, Loss: 0.740\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 95, Loss: 0.738\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 96, Loss: 0.735\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 97, Loss: 0.735\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 98, Loss: 0.733\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 99, Loss: 0.732\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 100, Loss: 0.729\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 101, Loss: 0.728\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 102, Loss: 0.728\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 103, Loss: 0.724\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 104, Loss: 0.727\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 105, Loss: 0.725\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 106, Loss: 0.722\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 107, Loss: 0.721\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 108, Loss: 0.721\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 109, Loss: 0.719\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 110, Loss: 0.718\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 111, Loss: 0.718\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 112, Loss: 0.718\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 113, Loss: 0.714\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 114, Loss: 0.711\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 115, Loss: 0.714\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 116, Loss: 0.711\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 117, Loss: 0.710\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 118, Loss: 0.707\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 119, Loss: 0.708\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 120, Loss: 0.705\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 121, Loss: 0.705\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 122, Loss: 0.705\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 123, Loss: 0.703\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 124, Loss: 0.701\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 125, Loss: 0.703\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 126, Loss: 0.701\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 127, Loss: 0.699\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 128, Loss: 0.700\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 129, Loss: 0.699\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 130, Loss: 0.698\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 131, Loss: 0.696\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 132, Loss: 0.696\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 133, Loss: 0.696\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 134, Loss: 0.691\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 135, Loss: 0.694\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 136, Loss: 0.691\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 137, Loss: 0.691\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 138, Loss: 0.689\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 139, Loss: 0.690\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 140, Loss: 0.689\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 141, Loss: 0.690\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 142, Loss: 0.687\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 143, Loss: 0.687\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 144, Loss: 0.686\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 145, Loss: 0.685\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 146, Loss: 0.684\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 147, Loss: 0.685\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 148, Loss: 0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 149, Loss: 0.682\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 150, Loss: 0.681\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 151, Loss: 0.681\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 152, Loss: 0.679\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 153, Loss: 0.678\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 154, Loss: 0.677\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 155, Loss: 0.677\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 156, Loss: 0.676\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 157, Loss: 0.677\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 158, Loss: 0.678\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 159, Loss: 0.675\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 160, Loss: 0.675\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 161, Loss: 0.673\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 162, Loss: 0.672\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 163, Loss: 0.673\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 164, Loss: 0.672\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 165, Loss: 0.670\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 166, Loss: 0.671\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 167, Loss: 0.671\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 168, Loss: 0.670\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 169, Loss: 0.669\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 170, Loss: 0.666\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 171, Loss: 0.666\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 172, Loss: 0.667\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 173, Loss: 0.666\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 174, Loss: 0.667\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 175, Loss: 0.668\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 176, Loss: 0.665\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 177, Loss: 0.663\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 178, Loss: 0.663\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 179, Loss: 0.664\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 180, Loss: 0.662\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 181, Loss: 0.660\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 182, Loss: 0.661\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 183, Loss: 0.660\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 184, Loss: 0.662\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 185, Loss: 0.658\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 186, Loss: 0.660\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 187, Loss: 0.660\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 188, Loss: 0.658\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 189, Loss: 0.658\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 190, Loss: 0.657\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 191, Loss: 0.659\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 192, Loss: 0.657\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 193, Loss: 0.655\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 194, Loss: 0.656\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 195, Loss: 0.654\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 196, Loss: 0.655\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 197, Loss: 0.653\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 198, Loss: 0.652\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 199, Loss: 0.652\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 200, Loss: 0.652\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 201, Loss: 0.653\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 202, Loss: 0.651\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 203, Loss: 0.650\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 204, Loss: 0.652\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 205, Loss: 0.650\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 206, Loss: 0.650\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 207, Loss: 0.648\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 208, Loss: 0.648\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 209, Loss: 0.648\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 210, Loss: 0.649\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 211, Loss: 0.648\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 212, Loss: 0.647\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 213, Loss: 0.646\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 214, Loss: 0.645\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 215, Loss: 0.645\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 216, Loss: 0.645\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 217, Loss: 0.644\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 218, Loss: 0.643\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 219, Loss: 0.644\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 220, Loss: 0.644\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 221, Loss: 0.642\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 222, Loss: 0.643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 223, Loss: 0.644\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 224, Loss: 0.643\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 225, Loss: 0.640\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 226, Loss: 0.641\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 227, Loss: 0.639\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 228, Loss: 0.641\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 229, Loss: 0.638\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 230, Loss: 0.639\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 231, Loss: 0.640\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 232, Loss: 0.640\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 233, Loss: 0.639\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 234, Loss: 0.638\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 235, Loss: 0.636\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 236, Loss: 0.636\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 237, Loss: 0.636\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 238, Loss: 0.638\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 239, Loss: 0.638\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 240, Loss: 0.635\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 241, Loss: 0.635\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 242, Loss: 0.634\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 243, Loss: 0.634\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 244, Loss: 0.633\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 245, Loss: 0.632\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 246, Loss: 0.634\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 247, Loss: 0.633\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 248, Loss: 0.634\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 249, Loss: 0.633\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 250, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 251, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 252, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 253, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 254, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 255, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 256, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 257, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 258, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 259, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 260, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 261, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 262, Loss: 0.623\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 263, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 264, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 265, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 266, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 267, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 268, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 269, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 270, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 271, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 272, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 273, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 274, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 275, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 276, Loss: 0.622\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 277, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 278, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 279, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 280, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 281, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 282, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 283, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 284, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 285, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 286, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 287, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 288, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 289, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 290, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 291, Loss: 0.621\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 292, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 293, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 294, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 295, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 296, Loss: 0.620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 297, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 298, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 299, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 300, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 301, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 302, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 303, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 304, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 305, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 306, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 307, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 308, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 309, Loss: 0.620\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 310, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 311, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 312, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 313, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 314, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 315, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 316, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 317, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 318, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 319, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 320, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 321, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 322, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 323, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 324, Loss: 0.619\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 325, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 326, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 327, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 328, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 329, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 330, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 331, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 332, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 333, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 334, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 335, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 336, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 337, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 338, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 339, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 340, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 341, Loss: 0.618\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 342, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 343, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 344, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 345, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 346, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 347, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 348, Loss: 0.617\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Pretraining Autoencoder... Epoch: 349, Loss: 0.617\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=250\n",
    "    num_epochs_ae=350\n",
    "    patience=100\n",
    "    lr=1e-4\n",
    "    weight_decay=0.5e-6\n",
    "    weight_decay_ae=0.5e-3\n",
    "    lr_ae=1e-4\n",
    "    lr_milestones_ae=[250]\n",
    "    lr_milestones=[150]\n",
    "    batch_size=200\n",
    "    pretrain=True\n",
    "    latent_dim=32\n",
    "    anormal_class=5\n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader_train, dataloader_val, dataloader_test = get_mnist(args)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, dataloader_train, dataloader_val, device)\n",
    "\n",
    "if args.pretrain:\n",
    "    deep_SVDD.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 0, Loss: 1.292\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 0, Loss: 0.357\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.240\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 1, Loss: 0.17\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.141\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 2, Loss: 0.115\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.101\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 3, Loss: 0.0859\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.078\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 4, Loss: 0.0675\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.064\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 5, Loss: 0.0563\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.055\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 6, Loss: 0.048\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.048\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 7, Loss: 0.0429\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.042\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 8, Loss: 0.0377\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.037\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 9, Loss: 0.0343\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.033\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 10, Loss: 0.0303\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.031\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 11, Loss: 0.0273\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.028\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 12, Loss: 0.0251\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.025\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 13, Loss: 0.0228\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.023\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 14, Loss: 0.0208\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.022\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 15, Loss: 0.0201\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.020\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 16, Loss: 0.0177\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.018\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 17, Loss: 0.0162\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.017\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 18, Loss: 0.0156\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.016\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 19, Loss: 0.0143\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.015\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 20, Loss: 0.0133\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.014\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 21, Loss: 0.0121\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.013\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 22, Loss: 0.0115\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.012\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 23, Loss: 0.0106\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.011\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 24, Loss: 0.01\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.010\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 25, Loss: 0.00945\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.010\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 26, Loss: 0.00876\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.009\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 27, Loss: 0.00817\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.009\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 28, Loss: 0.00783\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.008\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 29, Loss: 0.0072\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.008\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 30, Loss: 0.00693\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.007\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 31, Loss: 0.0067\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.007\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 32, Loss: 0.0061\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.006\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 33, Loss: 0.00577\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.006\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 34, Loss: 0.00543\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.006\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 35, Loss: 0.00507\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 36, Loss: 0.00488\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.005\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 37, Loss: 0.00481\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.005\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 38, Loss: 0.00451\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.005\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 39, Loss: 0.00411\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.004\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 40, Loss: 0.00389\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.004\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 41, Loss: 0.00375\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.004\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 42, Loss: 0.00354\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.004\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 43, Loss: 0.00326\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.004\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 44, Loss: 0.00312\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 45, Loss: 0.00312\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 46, Loss: 0.00283\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 47, Loss: 0.00275\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 48, Loss: 0.00281\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 49, Loss: 0.00267\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.003\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 50, Loss: 0.00223\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 51, Loss: 0.0023\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 52, Loss: 0.00205\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 53, Loss: 0.00198\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 54, Loss: 0.00183\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 55, Loss: 0.00182\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 56, Loss: 0.00166\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 57, Loss: 0.00162\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 58, Loss: 0.00159\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 59, Loss: 0.00152\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.002\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 60, Loss: 0.00133\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 61, Loss: 0.0013\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 62, Loss: 0.00125\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 63, Loss: 0.00123\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 64, Loss: 0.00116\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 65, Loss: 0.00146\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 66, Loss: 0.00102\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 67, Loss: 0.00099\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 68, Loss: 0.000978\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 69, Loss: 0.00109\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 70, Loss: 0.000803\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 71, Loss: 0.000788\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 72, Loss: 0.000724\n",
      "Weights saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 73, Loss: 0.00085\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 74, Loss: 0.000698\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 75, Loss: 0.00118\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 76, Loss: 0.000608\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 77, Loss: 0.000609\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 78, Loss: 0.000534\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 79, Loss: 0.000518\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 80, Loss: 0.000625\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 81, Loss: 0.000463\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.001\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 82, Loss: 0.000487\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 83, Loss: 0.000493\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 84, Loss: 0.000506\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 85, Loss: 0.000399\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 86, Loss: 0.00039\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 87, Loss: 0.000426\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 88, Loss: 0.000399\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 89, Loss: 0.00039\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 90, Loss: 0.000367\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 91, Loss: 0.000355\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 92, Loss: 0.000373\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 93, Loss: 0.000469\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 94, Loss: 0.000298\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 95, Loss: 0.000374\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 96, Loss: 0.000272\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 97, Loss: 0.00027\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 98, Loss: 0.000282\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 99, Loss: 0.000265\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 100, Loss: 0.000261\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 101, Loss: 0.000328\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 102, Loss: 0.000262\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 103, Loss: 0.000227\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 104, Loss: 0.000255\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 105, Loss: 0.000231\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 106, Loss: 0.000235\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 107, Loss: 0.00021\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 108, Loss: 0.000256\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 109, Loss: 0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 110, Loss: 0.00019\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 111, Loss: 0.000178\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 112, Loss: 0.000276\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 113, Loss: 0.000193\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 114, Loss: 0.000296\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 115, Loss: 0.000156\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 116, Loss: 0.000143\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 117, Loss: 0.000149\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 118, Loss: 0.000506\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 119, Loss: 0.000248\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 120, Loss: 0.000147\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 121, Loss: 0.000352\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 122, Loss: 0.000166\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 123, Loss: 0.0002\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 124, Loss: 0.000117\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 125, Loss: 0.000129\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 126, Loss: 0.000383\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 127, Loss: 0.000155\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 128, Loss: 0.000211\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 129, Loss: 0.00013\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 130, Loss: 0.000143\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 131, Loss: 0.000122\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 132, Loss: 0.000123\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 133, Loss: 0.000115\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 134, Loss: 0.000108\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 135, Loss: 0.000122\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 136, Loss: 0.000103\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 137, Loss: 0.0001\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 138, Loss: 0.000188\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 139, Loss: 0.000114\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 140, Loss: 0.000119\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 141, Loss: 0.000226\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 142, Loss: 0.000102\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 143, Loss: 0.000107\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 144, Loss: 9.17e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 145, Loss: 0.000122\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 146, Loss: 0.00017\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 147, Loss: 0.000103\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 148, Loss: 0.000104\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 149, Loss: 8.8e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 150, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 150, Loss: 6.22e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 151, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 151, Loss: 5.9e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 152, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 152, Loss: 6.84e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 153, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 153, Loss: 6.43e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 154, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 154, Loss: 6.28e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 155, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 155, Loss: 5.93e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 156, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 156, Loss: 5.79e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 157, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 157, Loss: 6.53e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 158, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 158, Loss: 5.8e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 159, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 159, Loss: 5.83e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 160, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 160, Loss: 7.36e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 161, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 161, Loss: 6.53e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 162, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 162, Loss: 5.94e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 163, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 163, Loss: 5.93e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 164, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 164, Loss: 5.96e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 165, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 165, Loss: 5.94e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 166, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 166, Loss: 5.95e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 167, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 167, Loss: 5.74e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 168, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 168, Loss: 6.93e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 169, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 169, Loss: 5.57e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 170, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 170, Loss: 5.99e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 171, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 171, Loss: 5.63e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 172, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 172, Loss: 5.65e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 173, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 173, Loss: 5.75e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 174, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 174, Loss: 5.54e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 175, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 175, Loss: 5.57e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 176, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 176, Loss: 5.73e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 177, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 177, Loss: 6.25e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 178, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 178, Loss: 5.91e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 179, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 179, Loss: 5.32e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 180, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 180, Loss: 5.52e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 181, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 181, Loss: 5.52e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 182, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 182, Loss: 5.84e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 183, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 183, Loss: 6.75e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 184, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 184, Loss: 5.33e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 185, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 185, Loss: 6.63e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 186, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 186, Loss: 5.46e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 187, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 187, Loss: 5.87e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 188, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 188, Loss: 5.26e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 189, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 189, Loss: 6.46e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 190, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 190, Loss: 6.06e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 191, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 191, Loss: 6.82e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 192, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 192, Loss: 5.18e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 193, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 193, Loss: 5.27e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 194, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 194, Loss: 5.14e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 195, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 195, Loss: 5.85e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 196, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 196, Loss: 5.18e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 197, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 197, Loss: 5.38e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 198, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 198, Loss: 5.28e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 199, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 199, Loss: 5.32e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 200, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 200, Loss: 5.02e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 201, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 201, Loss: 4.95e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 202, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 202, Loss: 4.98e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 203, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 203, Loss: 5.16e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 204, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 204, Loss: 5.71e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 205, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 205, Loss: 7.89e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 206, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 206, Loss: 6.11e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 207, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 207, Loss: 5.61e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 208, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 208, Loss: 5.24e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 209, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 209, Loss: 4.75e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 210, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 210, Loss: 4.98e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 211, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 211, Loss: 4.99e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 212, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 212, Loss: 4.96e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 213, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 213, Loss: 4.72e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 214, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 214, Loss: 5.46e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 215, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 215, Loss: 4.91e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 216, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 216, Loss: 5.29e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 217, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 217, Loss: 5.28e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 218, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 218, Loss: 5.02e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 219, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 219, Loss: 4.79e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 220, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 220, Loss: 4.77e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 221, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 221, Loss: 4.72e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 222, Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 222, Loss: 4.61e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 223, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 223, Loss: 4.63e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 224, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 224, Loss: 4.55e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 225, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 225, Loss: 4.73e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 226, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 226, Loss: 5.05e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 227, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 227, Loss: 4.73e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 228, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 228, Loss: 4.79e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 229, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 229, Loss: 5.65e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 230, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 230, Loss: 5.91e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 231, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 231, Loss: 4.71e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 232, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 232, Loss: 4.62e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 233, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 233, Loss: 4.92e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 234, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 234, Loss: 4.84e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 235, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 235, Loss: 4.66e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 236, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 236, Loss: 7.21e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 237, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 237, Loss: 4.84e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 238, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 238, Loss: 5.72e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 239, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 239, Loss: 4.79e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 240, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 240, Loss: 4.59e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 241, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 241, Loss: 4.49e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 242, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 242, Loss: 4.72e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 243, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 243, Loss: 4.83e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 244, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 244, Loss: 5.68e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 245, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 245, Loss: 4.72e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 246, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 246, Loss: 5.02e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 247, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 247, Loss: 4.5e-05\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 248, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 248, Loss: 4.37e-05\n",
      "Weights saved.\n",
      "43663/43663: [===============================>] - ETA 0.1ss\n",
      "Training Deep SVDD... Epoch: 249, Loss: 0.000\n",
      "10916/10916: [===============================>] - ETA 0.1s\n",
      "Testing Autoencoder... Epoch: 249, Loss: 4.84e-05\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 0.699\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval(net, c, dataloader, device):\n",
    "    \"\"\"Testing the Deep SVDD model\"\"\"\n",
    "\n",
    "    scores = []\n",
    "    labels1 = []\n",
    "    labels2 = []\n",
    "    net.eval()\n",
    "    print('Testing...')\n",
    "    with torch.no_grad():\n",
    "        for x, y1, y2 in dataloader:\n",
    "            x = x.float().to(device)\n",
    "            z = net(x)\n",
    "            score = torch.sum((z - c) ** 2, dim=1)\n",
    "\n",
    "            scores.append(score.detach().cpu())\n",
    "            labels1.append(y1.cpu())\n",
    "            labels2.append(y2.cpu())\n",
    "    labels1, labels2, scores = torch.cat(labels1).numpy(), torch.cat(labels2).numpy(), torch.cat(scores).numpy()\n",
    "    print('ROC AUC score: {:.3f}'.format(roc_auc_score(labels2, scores)))\n",
    "    return labels1, labels2, scores\n",
    "\n",
    "labels1, labels2, scores = eval(deep_SVDD.net, deep_SVDD.c, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in = scores[labels2==0]\n",
    "scores_out = scores[labels2==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb898258240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores_in, bins=50, color='b', alpha=0.3, density=True, label='Inlier')\n",
    "plt.hist(scores_out, bins=20, color='r', alpha=0.3, density=True, label='Outlier')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
